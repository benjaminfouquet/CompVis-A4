{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from classes.CNNModel import CNNModel\n",
    "from classes.TrainDataset import TrainDataset\n",
    "from classes.TestDataset import TestDataset\n",
    "from utils.cos_sim import cos_sim_make_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "- Use a pre-trained model for feature extraction\n",
    "- Make pre-trained/homemade model for feature processing\n",
    "- Compute 20*20 asymetric cosine similarity matrix\n",
    "- Select top 2 images most similar to 'left' image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'data/train/'\n",
    "test_folder = 'data/test/'\n",
    "csv_folder = 'data/'\n",
    "\n",
    "train_csv = csv_folder + 'train.csv'\n",
    "train_candidates_csv = csv_folder + 'train_candidates.csv'\n",
    "test_candidates_csv = csv_folder + 'test_candidates.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((49, 40)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the dataset and data loader\n",
    "train_dataset = TrainDataset(train_csv, train_candidates_csv, train_folder+'left', train_folder+'all', transform=transform)\n",
    "test_dataset = TestDataset(test_candidates_csv, test_folder+'left', test_folder+'all', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make baseline predictions, no models, no preprocessing. We use cosine similarity as our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 samples in 5.35 seconds\n",
      "Processed 400 samples in 9.44 seconds\n",
      "Processed 600 samples in 13.36 seconds\n",
      "Processed 800 samples in 17.21 seconds\n",
      "Processed 1000 samples in 21.03 seconds\n",
      "Processed 1200 samples in 25.17 seconds\n",
      "Processed 1400 samples in 29.05 seconds\n",
      "Processed 1600 samples in 33.11 seconds\n",
      "Processed 1800 samples in 37.12 seconds\n",
      "Processed 2000 samples in 41.24 seconds\n"
     ]
    }
   ],
   "source": [
    "df_output = cos_sim_make_output(test_loader, test_candidates_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: 0.10 on Kaggle, twice as much as random guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip ResNet 50x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import clip model (source: https://github.com/openai/CLIP). Python version 3.10 recommended "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=288, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(288, 288))\n",
       "    <function _convert_image_to_rgb at 0x0000024C4D9BD090>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clip import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model, transform = clip.load(\"RN50x4\", device)\n",
    "\n",
    "# Define our own transform, suitable for our problem\n",
    "t = transforms.Compose([transforms.ToPILImage(), \n",
    "                        transform, \n",
    "                        # add batch dimension\n",
    "                        transforms.Lambda(lambda x: x.unsqueeze(0))])\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of a visual model: Modified ResNet model (that we're going to use). A transformer, a token embedding and a final layer. Uncomment and execute next cell for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Modified resNet model is composed of 4 layers and an attention pool. We're going to extract the representation after each layer and concatenate them in one vector of size 6000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 60, 131, 246, 317]\n",
      "conv1 Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "bn1 BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu1 ReLU(inplace=True)\n",
      "conv2 Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "bn2 BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu2 ReLU(inplace=True)\n",
      "conv3 Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "bn3 BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu3 ReLU(inplace=True)\n",
      "avgpool AvgPool2d(kernel_size=2, stride=2, padding=0)\n"
     ]
    }
   ],
   "source": [
    "# names can be found in the detail of the model, have to get indices to get the modules\n",
    "layers_names = [\"layer1\", \"layer2\", \"layer3\", \"layer4\", \"attnpool\"]\n",
    "\n",
    "# get index of layer in the model by layer name\n",
    "def get_layer_index(model, layer_name):\n",
    "    return list(model.modules()).index(getattr(model, layer_name))\n",
    "\n",
    "# get indices\n",
    "layers_indices = [get_layer_index(model.visual, layer) for layer in layers_names]\n",
    "print(layers_indices)\n",
    "\n",
    "# Those are the modules we're interested in getting the output from\n",
    "modules = []\n",
    "for layer_index in layers_indices:\n",
    "    layer_name, module = list(model.visual.named_modules())[layer_index]\n",
    "    modules.append(module)\n",
    "\n",
    "# Those are the modules used before layer1, we need them to get the input of layer1\n",
    "# (they are the first 10 modules)\n",
    "pre_modules = []\n",
    "for layer_index in range(1, 11):\n",
    "    layer_name, pre_module = list(model.visual.named_modules())[layer_index]\n",
    "    pre_modules.append(pre_module)\n",
    "    print(layer_name, pre_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make PCA on training data\n",
    "import glob\n",
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def get_full_encoding(folder_path, pre_modules, modules, transform):\n",
    "    \"\"\"Get encoding for all images in folder_path. Concatenates the output \n",
    "    of the modules (not pre-modules). For outputs that are tensors with more than 2\n",
    "    dimensions (including batch_size dimension), take the average of the spatial dimensions.\n",
    "    Normalize the vectors with L2 norm.\n",
    "    \n",
    "    Encoded images are in he same order as the images in the folder.\"\"\"\n",
    "    images = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # One by one with blob\n",
    "    count = 0\n",
    "    for img_path in glob.glob(f'{folder_path}/*.jpg'):\n",
    "        # get and preprocess\n",
    "        img = mpimg.imread(img_path)\n",
    "        img_t = transform(img)\n",
    "\n",
    "        # layers for which we're not interested in the output\n",
    "        out = img_t\n",
    "        for pre_module in pre_modules:\n",
    "            out = pre_module(out)\n",
    "\n",
    "        # layers for which we're interested in the output\n",
    "        for i in range(len(modules)):\n",
    "            module = modules[i]\n",
    "            out = module(out)\n",
    "            if out.ndim == 4:\n",
    "                # Average spatial dimensions\n",
    "                vec = torch.mean(out, dim=(2, 3))\n",
    "                # L2 normalization\n",
    "                vec = vec / vec.norm(dim=-1, keepdim=True)\n",
    "            else:\n",
    "                vec = out\n",
    "                vec = vec / vec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # concatenate outputs\n",
    "            if i == 0:\n",
    "                img_encoding = vec\n",
    "            else:\n",
    "                img_encoding = torch.cat((img_encoding, vec), dim=1)\n",
    "\n",
    "        # Make numpy array from tensor\n",
    "        img_enc = img_encoding.detach().numpy().flatten()\n",
    "        images.append(img_enc)\n",
    "        count += 1\n",
    "    \n",
    "        if count%200 == 0:\n",
    "            print(f\"Processed: {count}, Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 200, Elapsed time:  60.352022647857666\n",
      "Processed: 400, Elapsed time:  57.6754264831543\n",
      "Processed: 600, Elapsed time:  60.924506187438965\n",
      "Processed: 800, Elapsed time:  61.80785918235779\n",
      "Processed: 1000, Elapsed time:  55.01278114318848\n",
      "Processed: 1200, Elapsed time:  56.16243553161621\n",
      "Processed: 1400, Elapsed time:  57.27299475669861\n",
      "Processed: 1600, Elapsed time:  56.95402932167053\n",
      "Processed: 1800, Elapsed time:  57.45021677017212\n",
      "Processed: 2000, Elapsed time:  57.05836224555969\n",
      "Processed: 2200, Elapsed time:  57.81105041503906\n",
      "Processed: 2400, Elapsed time:  57.90114736557007\n",
      "Processed: 2600, Elapsed time:  59.954755783081055\n",
      "Processed: 2800, Elapsed time:  60.240116119384766\n",
      "Processed: 3000, Elapsed time:  58.31308674812317\n",
      "Processed: 3200, Elapsed time:  59.88385820388794\n",
      "Processed: 3400, Elapsed time:  57.90711307525635\n",
      "Processed: 3600, Elapsed time:  58.74224328994751\n",
      "Processed: 3800, Elapsed time:  58.89434480667114\n",
      "Processed: 4000, Elapsed time:  56.58669114112854\n"
     ]
    }
   ],
   "source": [
    "# Expect 25 mins of processing. Encode all training images for validation\n",
    "images = get_full_encoding(\"data/train/all\", pre_modules, modules, t)\n",
    "images_np = np.array(images)\n",
    "np.save('data/train_all_full_encodings.npy', images_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "# Embedded vectors have 6000 dimensions, we reduce to 256 (93% variance explained)\n",
    "pca = PCA(n_components=256)\n",
    "pca.fit(images)\n",
    "images_pca = pca.transform(images)\n",
    "sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Save the model\n",
    "with open('models/pca_module.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 200, Elapsed time:  60.38086676597595\n",
      "Processed: 400, Elapsed time:  57.38793420791626\n",
      "Processed: 600, Elapsed time:  58.32379364967346\n",
      "Processed: 800, Elapsed time:  59.23546552658081\n",
      "Processed: 1000, Elapsed time:  58.334389209747314\n",
      "Processed: 1200, Elapsed time:  58.5935435295105\n",
      "Processed: 1400, Elapsed time:  59.32722544670105\n",
      "Processed: 1600, Elapsed time:  59.9698121547699\n",
      "Processed: 1800, Elapsed time:  58.60418176651001\n",
      "Processed: 2000, Elapsed time:  58.26839780807495\n",
      "Processed: 2200, Elapsed time:  57.99907374382019\n",
      "Processed: 2400, Elapsed time:  57.608824729919434\n",
      "Processed: 2600, Elapsed time:  57.610082387924194\n",
      "Processed: 2800, Elapsed time:  57.27380180358887\n",
      "Processed: 3000, Elapsed time:  58.05987501144409\n",
      "Processed: 3200, Elapsed time:  59.79752492904663\n",
      "Processed: 3400, Elapsed time:  57.83853077888489\n",
      "Processed: 3600, Elapsed time:  58.522308349609375\n",
      "Processed: 3800, Elapsed time:  57.76077127456665\n",
      "Processed: 4000, Elapsed time:  57.74093961715698\n"
     ]
    }
   ],
   "source": [
    "# Expect 25 mins of processing. Encode all test images. We'll use the PCA fit on training data\n",
    "images = get_full_encoding(\"data/test/all\", pre_modules, modules, t)\n",
    "images_np = np.array(images)\n",
    "np.save('data/test_all_full_encodings.npy', images_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adaptator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with training data for validation score\n",
    "images = np.load('data/train_all_full_encodings.npy')\n",
    "pca = pickle.load(open('models/pca_module.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded images\n",
    "train_images_enc = np.load('data/train_all_full_encodings.npy')\n",
    "test_images_enc = np.load('data/test_all_full_encodings.npy')\n",
    "\n",
    "# Read candidates\n",
    "test_candidates = pd.read_csv(test_candidates_csv)\n",
    "train_candidates = pd.read_csv(train_candidates_csv)\n",
    "\n",
    "# Get images names\n",
    "test_images_names = np.array([x.split('.')[0] for x in os.listdir('data/test/all')])\n",
    "train_images_names = np.array([x.split('.')[0] for x in os.listdir('data/train/all')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cos_sim_np(left, right_images):\n",
    "    \"\"\"Compute cosine similarity between left image and all right images in row\"\"\"\n",
    "    return np.array([cosine_similarity(left, right_images[i])[0][0] for i in range(len(right_images))])\n",
    "\n",
    "def cos_sim_enc(images_enc, images_names, candidates_csv):\n",
    "    \"\"\"returns a data frame with format suitable for submitting, except that values \n",
    "    don't represent probabilities but cosine similarities.\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, row in candidates_csv.iterrows():\n",
    "\n",
    "        # Compute cosine similarity between left and all other images in row\n",
    "        left = images_enc[np.where(images_names == row['left'])]\n",
    "        right_images = np.array([images_enc[np.where(images_names == row[f'c{i}'])] for i in range(20)])\n",
    "        sim_array = cos_sim_np(left, right_images)\n",
    "        res = [row['left']] + list(sim_array)\n",
    "        results.append(list(res))\n",
    "\n",
    "        if i%400 == 0 and i!=0:\n",
    "            print(f\"Processed: {i}\")\n",
    "            print(\"Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            \n",
    "    results = np.array(results)\n",
    "    column_names = ['left'] + [f'c{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Functions to make dataframes values into probabilities\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x (usually a row of dataframe).\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def make_df_as_probs(df):\n",
    "    # Make the values into probabilities with softmax\n",
    "    df_as_probs = df.apply(lambda row: softmax(np.array(row[1:]).astype(np.float64)), axis=1)\n",
    "    # Make into new dataframe\n",
    "    df_as_probs = pd.DataFrame(df_as_probs.values.tolist(), columns=df.columns[1:])\n",
    "    # Add left column in first position\n",
    "    df_as_probs.insert(0, 'left', df['left'])\n",
    "    \n",
    "    return df_as_probs\n",
    "\n",
    "# Evaluate result for training data (gives an idea of performance). For each row in df_train get \n",
    "# index of colmuns of top 2 values and check if the true match is in the top 2\n",
    "def get_score(row, row_nb, true_labels, candidates):\n",
    "    \"\"\"return 1 if true label in top_2, else return 0\"\"\"\n",
    "    # Get top 2\n",
    "    top_2 = row[1:].argsort()[-2:][::-1].values\n",
    "\n",
    "    # Get true label\n",
    "    label_row = true_labels[true_labels['left'] == row['left']]\n",
    "    true_label = label_row['right'].values[0]\n",
    "\n",
    "    # Get top 2 predicted labels. i+1 to account for 'left' column\n",
    "    top_2_names = [candidates.iloc[row_nb, i+1] for i in top_2]\n",
    "    if true_label in top_2_names:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def eval(df, true_labels, candidates):\n",
    "    \"\"\"Evaluate score on df\"\"\"\n",
    "    score = 0 \n",
    "    for i, row in df.iterrows():\n",
    "        score += get_score(row, i, true_labels, candidates)\n",
    "    return score/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 400\n",
      "Elapsed time:  2.1653501987457275\n",
      "Processed: 800\n",
      "Elapsed time:  2.0913102626800537\n",
      "Processed: 1200\n",
      "Elapsed time:  2.081334114074707\n",
      "Processed: 1600\n",
      "Elapsed time:  2.110325813293457\n",
      "Processed: 400\n",
      "Elapsed time:  2.1007297039031982\n",
      "Processed: 800\n",
      "Elapsed time:  2.1041040420532227\n",
      "Processed: 1200\n",
      "Elapsed time:  2.123558282852173\n",
      "Processed: 1600\n",
      "Elapsed time:  2.1378159523010254\n",
      "Accuracy:  63.55%\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA on embedded images\n",
    "images_train_pca = pca.transform(train_images_enc)\n",
    "images_test_pca = pca.transform(test_images_enc)\n",
    "\n",
    "# Get dataframe with cosine similarities between left image and candidates\n",
    "df_train = cos_sim_enc(images_train_pca, train_images_names, train_candidates)\n",
    "df_test = cos_sim_enc(images_test_pca, test_images_names, test_candidates)\n",
    "\n",
    "# Make the values into probabilities with softmax\n",
    "df_train_as_probs = make_df_as_probs(df_train)\n",
    "df_test_as_probs = make_df_as_probs(df_test)\n",
    "\n",
    "# Evaluate performance on training set\n",
    "train_labels = pd.read_csv('data/train.csv')\n",
    "print(f'Accuracy:  {eval(df_train_as_probs, train_labels, train_candidates)*100}%')\n",
    "\n",
    "# Write csv\n",
    "df_test_as_probs.to_csv('output/cos_sim_full_enc_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of 64% accuracy is compelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few approaches to try for improving the model:\n",
    "-   Preprocessing the input. Right now we use the transform provided with the model. The resizing and cropping are necessary for the model, but the normalization is standard for images from ImageNet. Our dataset comes from the reddit TTL, we might look into deriving the mean and std values from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=288, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(288, 288))\n",
       "    <function _convert_image_to_rgb at 0x0000024C4D9BD090>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Finetuning values: is 93% variance too much ? If we reduce the number of components in the PCA, will that improve or hurt the performance, the generalisation. To explore.\n",
    "-   Include more layers output in the representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
