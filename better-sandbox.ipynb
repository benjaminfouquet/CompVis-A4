{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision) (2.30.0)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (4.65.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from ftfy) (0.2.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\hec\\appdata\\local\\temp\\pip-req-build-j3xl6vew\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from ftfy->clip==1.0) (0.2.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch->clip==1.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision->clip==1.0) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision->clip==1.0) (2.30.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torchvision->clip==1.0) (10.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Hec\\AppData\\Local\\Temp\\pip-req-build-j3xl6vew'\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.11.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (2.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (68.1.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (2.3.7)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard) (0.41.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from markdown>=2.6.8->tensorboard) (6.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hec\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: C:\\Users\\Hec\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hec\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from classes.adaptation import calc_loss, construct_model\n",
    "from classes.asymmetricRecall import AsymmetricRecall\n",
    "from classes.embDataset import EmbDataset\n",
    "\n",
    "from utils.adaptationPreprocess import create_paired_embeddings_dict, merge_dicts_with_csv\n",
    "from utils.train_functions import train, validate, split_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_FILES_PATH = 'data/train/left/*'\n",
    "RIGHT_FILES_PATH = 'data/train/right/*'\n",
    "ALL_FILES_PATH = 'data/train/all/*'\n",
    "ALL_ENCODINGS_PATH = 'data/train_all_encodings.npy'\n",
    "ALL_FULL_ENCODINGS_PATH = 'data/train_all_full_encodings.npy'\n",
    "CSV_FILENAME = 'data/train.csv'\n",
    "SAVE_TO = 'output/'\n",
    "\n",
    "TEST_LEFT_FILES_PATH = 'data/test/left/*'\n",
    "TEST_RIGHT_FILES_PATH = 'data/test/right/*'\n",
    "TEST_ALL_FILES_PATH = 'data/test/all/*'\n",
    "TEST_ALL_FULL_ENCODINGS_PATH = 'data/test_all_full_encodings.npy'\n",
    "TEST_CSV_FILENAME = 'data/test_candidates.csv'\n",
    "\n",
    "train_csv = 'data/train.csv'\n",
    "train_candidates_csv = 'data/train_candidates.csv'\n",
    "test_candidates_csv = 'data/test_candidates.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if not is_cuda:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "temperature = 1\n",
    "topk = [i for i in range(1, 21)]\n",
    "epochs = 20\n",
    "test_split = 0.8\n",
    "model_name = 'original'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import peviously computed encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All encodings shape:  (4000, 640)\n",
      "All full encodings shape: (4000, 5440)\n"
     ]
    }
   ],
   "source": [
    "all_encodings = np.load(ALL_ENCODINGS_PATH)\n",
    "all_full_encodings = np.load(ALL_FULL_ENCODINGS_PATH)\n",
    "\n",
    "dimensions_before_pca = all_encodings.shape[1]\n",
    "print('All encodings shape: ', all_encodings.shape)\n",
    "print('All full encodings shape:', all_full_encodings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match embeddings with their original image name, link the left image with the ground truth right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hec\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator PCA from version 1.3.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start with training data for validation score\n",
    "images = np.load('data/train_all_full_encodings.npy')\n",
    "pca = pickle.load(open('models/pca_module.pkl', 'rb'))\n",
    "# Load encoded images\n",
    "train_images_enc = np.load('data/train_all_full_encodings.npy')\n",
    "test_images_enc = np.load('data/test_all_full_encodings.npy')\n",
    "\n",
    "# Read candidates\n",
    "test_candidates = pd.read_csv(test_candidates_csv)\n",
    "train_candidates = pd.read_csv(train_candidates_csv)\n",
    "\n",
    "# Get images names\n",
    "test_images_names = np.array([x.split('.')[0] for x in os.listdir('data/test/all')])\n",
    "train_images_names = np.array([x.split('.')[0] for x in os.listdir('data/train/all')])\n",
    "\n",
    "# Apply PCA on embedded images\n",
    "images_train_pca = pca.transform(train_images_enc)\n",
    "images_test_pca = pca.transform(test_images_enc)\n",
    "\n",
    "# # Get dataframe with cosine similarities between left image and candidates\n",
    "# df_train = cos_sim_enc(images_train_pca, train_images_names, train_candidates)\n",
    "# df_test = cos_sim_enc(images_test_pca, test_images_names, test_candidates)\n",
    "\n",
    "# # Make the values into probabilities with softmax\n",
    "# df_train_as_probs = make_df_as_probs(df_train)\n",
    "# df_test_as_probs = make_df_as_probs(df_test)\n",
    "\n",
    "# # Evaluate performance on training set\n",
    "# train_labels = pd.read_csv('data/train.csv')\n",
    "# eval(df_train_as_probs, train_labels, train_candidates)\n",
    "\n",
    "# # Write csv\n",
    "# df_test_as_probs.to_csv('output/cos_sim_full_enc_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 256)\n"
     ]
    }
   ],
   "source": [
    "paired_embeddings = create_paired_embeddings_dict(images_train_pca, ALL_FILES_PATH, LEFT_FILES_PATH, RIGHT_FILES_PATH)\n",
    "\n",
    "merged_dicts = merge_dicts_with_csv(CSV_FILENAME, paired_embeddings)\n",
    "print(merged_dicts['left'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define et construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model(model_name, 256).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:   2000\n",
      "train set without aug size  200\n",
      "train set with    aug size  200\n",
      "test  set without aug size   800\n"
     ]
    }
   ],
   "source": [
    "embds_dataset = EmbDataset(merged_dicts, only_original=False)\n",
    "\n",
    "print(\"Total dataset size:  \", len(embds_dataset))\n",
    "in_dim = embds_dataset[0][0][\"left\"].shape[0]\n",
    "\n",
    "splitted = split_train_test(embds_dataset, test_split, device)\n",
    "train_set, valid_set = splitted[\"train\"], splitted[\"test\"]\n",
    "\n",
    "print(\"train set without aug size \", len(train_set[0][\"left\"]))\n",
    "print(\"train set with    aug size \", len(train_set[1][\"left\"]))\n",
    "print(\"test  set without aug size  \", len(valid_set[\"left\"]))\n",
    "\n",
    "metrics_of_all_runs = []\n",
    "\n",
    "info = (\n",
    "    \"Embd path: \"\n",
    "    + str(ALL_FULL_ENCODINGS_PATH)\n",
    "    + \"\\n\"\n",
    "    + \"Using augmented images: \"\n",
    "    + str(embds_dataset.augmented)\n",
    "    + \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, save it, and get the metrics/loss for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 / 20\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20 / 20\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.12it/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.45s/it]\n"
     ]
    }
   ],
   "source": [
    "for run in trange(runs):\n",
    "    run_path = Path(SAVE_TO) / f\"run{run+1}\"\n",
    "    model = construct_model(model_name, in_dim).to(device)\n",
    "\n",
    "    writer = SummaryWriter(run_path)\n",
    "    writer.add_text(\"Temperature: \", str(temperature))\n",
    "    writer.add_text(\"Model\", str(model).replace(\"\\n\", \"  \\n\"))\n",
    "    writer.add_text(\"Informations: \", info.replace(\"\\n\", \"  \\n\"))\n",
    "\n",
    "    if model != \"dummy\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        # writer.add_text(\"Optimizer\", str(optimizer).replace(\"\\n\", \"  \\n\"))\n",
    "\n",
    "        metrics_per_run = train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            calc_loss,\n",
    "            train_set,\n",
    "            valid_set,\n",
    "            epochs,\n",
    "            run_path,\n",
    "            temperature,\n",
    "            topk,\n",
    "            device,\n",
    "        )\n",
    "        # save the model and the optimizer state_dics for this run\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epochs,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimzier\": optimizer.state_dict(),\n",
    "            },\n",
    "            run_path / \"model_and_optimizer.pt\",\n",
    "        )\n",
    "    else:\n",
    "        metrics_per_run = validate(\n",
    "            model,\n",
    "            calc_loss,\n",
    "            valid_set,\n",
    "            temperature,\n",
    "            topk,\n",
    "            device,\n",
    "        )\n",
    "        for metric, val in metrics_per_run.items():\n",
    "            writer.add_scalar(metric, val, 1)\n",
    "            writer.flush()\n",
    "            \n",
    "\n",
    "    np.save(run_path / \"data\", metrics_per_run)\n",
    "\n",
    "    metrics_of_all_runs.append(metrics_per_run)\n",
    "\n",
    "    splitted = split_train_test(embds_dataset, test_split, device)\n",
    "    train_set, valid_set = splitted[\"train\"], splitted[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all runs metrics into one file\n",
    "np.save(Path(SAVE_TO) / \"all_runs\", metrics_of_all_runs)\n",
    "\n",
    "avg_path = Path(SAVE_TO) / \"avg\"\n",
    "writer = SummaryWriter(avg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the saved metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'Train/Loss': [5.124790191650391, 5.104914665222168, 5.085371017456055, 5.066212177276611, 5.047489166259766, 5.029247283935547, 5.011515140533447, 4.994318962097168, 4.977666854858398, 4.961581230163574, 4.946057319641113, 4.931093215942383, 4.916676998138428, 4.902804851531982, 4.889459609985352, 4.876623153686523, 4.8642706871032715, 4.852385997772217, 4.840950012207031, 4.829952239990234], 'Train/top 1': [0.35, 0.395, 0.435, 0.495, 0.545, 0.6, 0.63, 0.675, 0.7, 0.735, 0.76, 0.805, 0.83, 0.835, 0.86, 0.875, 0.885, 0.91, 0.92, 0.93], 'Train/top 2': [0.43, 0.46, 0.535, 0.58, 0.615, 0.685, 0.705, 0.74, 0.775, 0.83, 0.865, 0.885, 0.905, 0.905, 0.91, 0.93, 0.945, 0.96, 0.96, 0.96], 'Train/top 3': [0.47, 0.545, 0.585, 0.63, 0.67, 0.73, 0.75, 0.79, 0.84, 0.865, 0.885, 0.91, 0.92, 0.925, 0.935, 0.95, 0.965, 0.97, 0.975, 0.98], 'Train/top 4': [0.495, 0.555, 0.61, 0.66, 0.705, 0.755, 0.79, 0.83, 0.87, 0.885, 0.905, 0.915, 0.93, 0.935, 0.945, 0.955, 0.965, 0.975, 0.985, 0.99], 'Train/top 5': [0.53, 0.585, 0.625, 0.675, 0.725, 0.765, 0.825, 0.845, 0.885, 0.905, 0.91, 0.925, 0.935, 0.945, 0.95, 0.96, 0.975, 0.98, 0.985, 1.0], 'Train/top 6': [0.55, 0.61, 0.665, 0.695, 0.745, 0.8, 0.84, 0.87, 0.895, 0.915, 0.92, 0.935, 0.95, 0.95, 0.975, 0.975, 0.975, 0.985, 1.0, 1.0], 'Train/top 7': [0.575, 0.62, 0.67, 0.715, 0.78, 0.815, 0.85, 0.89, 0.905, 0.92, 0.93, 0.95, 0.96, 0.97, 0.975, 0.975, 0.98, 0.99, 1.0, 1.0], 'Train/top 8': [0.59, 0.65, 0.69, 0.73, 0.795, 0.835, 0.875, 0.895, 0.92, 0.92, 0.935, 0.95, 0.965, 0.975, 0.98, 0.985, 0.99, 0.995, 1.0, 1.0], 'Train/top 9': [0.6, 0.655, 0.695, 0.755, 0.81, 0.855, 0.88, 0.905, 0.92, 0.93, 0.945, 0.965, 0.975, 0.98, 0.985, 0.99, 0.995, 1.0, 1.0, 1.0], 'Train/top 10': [0.605, 0.665, 0.7, 0.785, 0.815, 0.87, 0.885, 0.915, 0.925, 0.94, 0.955, 0.975, 0.975, 0.98, 0.985, 0.99, 0.995, 1.0, 1.0, 1.0], 'Train/top 11': [0.62, 0.69, 0.735, 0.785, 0.825, 0.88, 0.895, 0.925, 0.93, 0.94, 0.965, 0.975, 0.98, 0.985, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0], 'Train/top 12': [0.63, 0.69, 0.755, 0.82, 0.845, 0.885, 0.915, 0.925, 0.945, 0.95, 0.97, 0.975, 0.98, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 13': [0.635, 0.695, 0.765, 0.82, 0.86, 0.895, 0.915, 0.93, 0.945, 0.955, 0.97, 0.98, 0.98, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 14': [0.635, 0.71, 0.775, 0.825, 0.875, 0.9, 0.925, 0.935, 0.945, 0.955, 0.97, 0.98, 0.98, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 15': [0.66, 0.725, 0.78, 0.84, 0.885, 0.91, 0.93, 0.935, 0.955, 0.96, 0.97, 0.98, 0.985, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 16': [0.665, 0.745, 0.785, 0.85, 0.885, 0.91, 0.93, 0.94, 0.955, 0.97, 0.97, 0.985, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 17': [0.685, 0.75, 0.795, 0.865, 0.89, 0.915, 0.935, 0.94, 0.955, 0.97, 0.975, 0.99, 0.995, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 18': [0.7, 0.75, 0.81, 0.865, 0.905, 0.925, 0.94, 0.94, 0.955, 0.97, 0.98, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 19': [0.71, 0.765, 0.815, 0.87, 0.91, 0.935, 0.94, 0.945, 0.955, 0.97, 0.99, 0.99, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Train/top 20': [0.72, 0.775, 0.84, 0.87, 0.925, 0.94, 0.94, 0.95, 0.96, 0.975, 0.99, 0.995, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'test': {'Test/Loss': [6.502102375030518, 6.497756004333496, 6.493418216705322, 6.489097595214844, 6.4848175048828125, 6.480597972869873, 6.476451873779297, 6.472386360168457, 6.468418121337891, 6.464550018310547, 6.460785865783691, 6.4571309089660645, 6.453590393066406, 6.450162887573242, 6.446855545043945, 6.443667888641357, 6.440596103668213, 6.437643051147461, 6.434810161590576, 6.432092189788818], 'Test/top 1': [0.23875, 0.24125, 0.24875, 0.25375, 0.25625, 0.2575, 0.26, 0.26125, 0.2625, 0.2625, 0.27, 0.27125, 0.27375, 0.2775, 0.28125, 0.28625, 0.29125, 0.2925, 0.295, 0.29625], 'Test/top 2': [0.28625, 0.28625, 0.28625, 0.29375, 0.2975, 0.3025, 0.305, 0.31, 0.31875, 0.3275, 0.33375, 0.34, 0.3475, 0.35, 0.3525, 0.35375, 0.35875, 0.36125, 0.36625, 0.36875], 'Test/top 3': [0.3075, 0.31, 0.3175, 0.32625, 0.32875, 0.34, 0.34125, 0.35125, 0.3625, 0.36875, 0.3725, 0.37375, 0.3775, 0.385, 0.3875, 0.39375, 0.40125, 0.41125, 0.41625, 0.41875], 'Test/top 4': [0.3375, 0.3425, 0.35125, 0.3525, 0.3625, 0.3675, 0.375, 0.38125, 0.39125, 0.39875, 0.40125, 0.41, 0.41875, 0.4275, 0.42625, 0.43, 0.43, 0.43875, 0.4475, 0.45375], 'Test/top 5': [0.3625, 0.37, 0.375, 0.38, 0.38375, 0.38625, 0.3975, 0.41, 0.41625, 0.425, 0.435, 0.445, 0.4525, 0.455, 0.46125, 0.46375, 0.46875, 0.47375, 0.48, 0.48125], 'Test/top 6': [0.37625, 0.37875, 0.38375, 0.39375, 0.4, 0.41, 0.425, 0.43375, 0.44125, 0.44875, 0.4525, 0.46125, 0.4675, 0.475, 0.47625, 0.4825, 0.4925, 0.49875, 0.5025, 0.51], 'Test/top 7': [0.39, 0.39125, 0.4025, 0.41125, 0.41625, 0.4325, 0.4425, 0.45375, 0.455, 0.4625, 0.4675, 0.47625, 0.485, 0.495, 0.5025, 0.5125, 0.5125, 0.51625, 0.51875, 0.52625], 'Test/top 8': [0.39875, 0.40875, 0.415, 0.425, 0.4375, 0.45125, 0.4625, 0.465, 0.47375, 0.4825, 0.48625, 0.4925, 0.50375, 0.5075, 0.5175, 0.5225, 0.5275, 0.53, 0.5375, 0.53875], 'Test/top 9': [0.41375, 0.42125, 0.435, 0.44875, 0.46, 0.46875, 0.475, 0.47875, 0.48375, 0.4925, 0.49875, 0.50625, 0.5125, 0.52, 0.535, 0.53625, 0.54, 0.5425, 0.54875, 0.5525], 'Test/top 10': [0.4275, 0.44125, 0.4525, 0.47, 0.475, 0.4825, 0.48875, 0.4975, 0.50375, 0.50875, 0.5125, 0.52125, 0.53125, 0.54125, 0.54625, 0.55125, 0.5525, 0.56, 0.56125, 0.5625], 'Test/top 11': [0.44875, 0.45625, 0.4675, 0.48375, 0.4875, 0.48875, 0.49875, 0.5125, 0.5175, 0.52875, 0.5275, 0.54, 0.5425, 0.5525, 0.5625, 0.5625, 0.56, 0.5675, 0.57125, 0.5775], 'Test/top 12': [0.45625, 0.47, 0.48125, 0.49125, 0.50375, 0.51, 0.5125, 0.5175, 0.5275, 0.5325, 0.54, 0.54875, 0.5475, 0.55875, 0.57, 0.5725, 0.575, 0.57375, 0.58125, 0.5925], 'Test/top 13': [0.47, 0.48125, 0.49875, 0.5025, 0.51, 0.515, 0.53, 0.53125, 0.53625, 0.5425, 0.55625, 0.56125, 0.565, 0.57125, 0.57375, 0.58, 0.5825, 0.5875, 0.59625, 0.6], 'Test/top 14': [0.47875, 0.4875, 0.505, 0.5075, 0.515, 0.525, 0.53375, 0.535, 0.54125, 0.55625, 0.56625, 0.57, 0.57375, 0.58125, 0.585, 0.58875, 0.6, 0.60875, 0.61, 0.61375], 'Test/top 15': [0.485, 0.50125, 0.50875, 0.5175, 0.52125, 0.5325, 0.53875, 0.545, 0.5525, 0.5625, 0.5725, 0.5775, 0.585, 0.59, 0.59625, 0.6, 0.60875, 0.61375, 0.61625, 0.6225], 'Test/top 16': [0.49, 0.50875, 0.515, 0.52, 0.52875, 0.5425, 0.545, 0.55, 0.56125, 0.575, 0.58125, 0.58625, 0.5975, 0.6, 0.60375, 0.61125, 0.61625, 0.6225, 0.625, 0.635], 'Test/top 17': [0.5025, 0.51625, 0.52, 0.53, 0.54125, 0.55125, 0.55625, 0.56125, 0.5725, 0.58, 0.58875, 0.59125, 0.6, 0.6075, 0.6125, 0.62125, 0.62375, 0.62375, 0.63375, 0.6425], 'Test/top 18': [0.51, 0.52, 0.5275, 0.53625, 0.54625, 0.55625, 0.56375, 0.57, 0.58125, 0.58875, 0.59375, 0.5975, 0.605, 0.61125, 0.62, 0.62625, 0.63, 0.62875, 0.63875, 0.64875], 'Test/top 19': [0.51625, 0.53, 0.53375, 0.545, 0.55375, 0.5625, 0.57125, 0.5775, 0.59, 0.595, 0.5975, 0.60375, 0.61125, 0.61875, 0.62375, 0.62875, 0.63125, 0.63875, 0.64875, 0.6525], 'Test/top 20': [0.52125, 0.5325, 0.54125, 0.55375, 0.55875, 0.57, 0.575, 0.585, 0.595, 0.59875, 0.60375, 0.615, 0.62375, 0.6275, 0.62875, 0.63375, 0.6375, 0.65, 0.655, 0.6625]}}\n"
     ]
    }
   ],
   "source": [
    "test = np.load('output/run1/data.npy', allow_pickle=True)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test_dataset, and the previously saved model. We get the prediction of the model on the test dataset, then post process it before submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cos_sim_np(left, right_images):\n",
    "    \"\"\"Compute cosine similarity between left image and all right images in row\"\"\"\n",
    "    return np.array([cosine_similarity(left, right_images[i])[0][0] for i in range(len(right_images))])\n",
    "\n",
    "def cos_sim_enc(images_enc, images_names, candidates_csv):\n",
    "    \"\"\"returns a data frame with format suitable for submitting, except that values \n",
    "    don't represent probabilities but cosine similarities.\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, row in candidates_csv.iterrows():\n",
    "\n",
    "        # Compute cosine similarity between left and all other images in row\n",
    "        left = images_enc[np.where(images_names == row['left'])]\n",
    "        right_images = np.array([images_enc[np.where(images_names == row[f'c{i}'])] for i in range(20)])\n",
    "        sim_array = cos_sim_np(left, right_images)\n",
    "        res = [row['left']] + list(sim_array)\n",
    "        results.append(list(res))\n",
    "\n",
    "        if i%400 == 0 and i!=0:\n",
    "            print(f\"Processed: {i}\")\n",
    "            print(\"Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            \n",
    "    results = np.array(results)\n",
    "    column_names = ['left'] + [f'c{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Functions to make dataframes values into probabilities\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x (usually a row of dataframe).\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def make_df_as_probs(df):\n",
    "    # Make the values into probabilities with softmax\n",
    "    df_as_probs = df.apply(lambda row: softmax(np.array(row[1:]).astype(np.float64)), axis=1)\n",
    "    # Make into new dataframe\n",
    "    df_as_probs = pd.DataFrame(df_as_probs.values.tolist(), columns=df.columns[1:])\n",
    "    # Add left column in first position\n",
    "    df_as_probs.insert(0, 'left', df['left'])\n",
    "    \n",
    "    return df_as_probs\n",
    "\n",
    "# Evaluate result for training data (gives an idea of performance). For each row in df_train get \n",
    "# index of colmuns of top 2 values and check if the true match is in the top 2\n",
    "def get_score(row, row_nb, true_labels, candidates):\n",
    "    \"\"\"return 1 if true label in top_2, else return 0\"\"\"\n",
    "    # Get top 2\n",
    "    top_2 = row[1:].argsort()[-2:][::-1].values\n",
    "\n",
    "    # Get true label\n",
    "    label_row = true_labels[true_labels['left'] == row['left']]\n",
    "    true_label = label_row['right'].values[0]\n",
    "\n",
    "    # Get top 2 predicted labels. i+1 to account for 'left' column\n",
    "    top_2_names = [candidates.iloc[row_nb, i+1] for i in top_2]\n",
    "    if true_label in top_2_names:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def eval(df, true_labels, candidates):\n",
    "    \"\"\"Evaluate score on df\"\"\"\n",
    "    score = 0 \n",
    "    for i, row in df.iterrows():\n",
    "        score += get_score(row, i, true_labels, candidates)\n",
    "    return score/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'classes.adaptation.Adaptation'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adaptation(\n",
       "  (weight_matrix): Linear(in_features=256, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(model))\n",
    "model.eval()\n",
    "\n",
    "# output = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Calculate the similarity matrix given two lists of embeddings.\n",
    "    added eps for numerical stability.\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loss(left, right, temp, device):\n",
    "    \"\"\"\n",
    "    Our loss function that is used in training.\n",
    "    \"\"\"\n",
    "    sim1 = new_sim_matrix(left, right)\n",
    "    sim2 = sim1.t()\n",
    "\n",
    "    num_classes = 20\n",
    "    one_hot_encoded_list = torch.tensor(np.array([np.eye(num_classes)[i - 1] for i in range(1, num_classes + 1)]))\n",
    "    # print(torch.arange(len(sim1)).long().to(device))\n",
    "    loss_left2right = F.cross_entropy(\n",
    "        sim1 * temp, one_hot_encoded_list\n",
    "    )\n",
    "    loss_right2left = F.cross_entropy(\n",
    "        sim2 * temp, one_hot_encoded_list\n",
    "    )\n",
    "    loss = loss_left2right * 0.5 + loss_right2left * 0.5\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class NewAsymmetricRecall:\n",
    "    \"\"\"\n",
    "    This class is responsible for calculating the topk matches to our query.\n",
    "    \"\"\"\n",
    "    def __init__(self, left_ebds, right_ebds):\n",
    "\n",
    "        self.M = cosine_similarity(left_ebds, right_ebds)\n",
    "        self.sorted_idx_1 = self.M.argsort(1)\n",
    "        self.sorted_idx_2 = self.M.T.argsort(1)\n",
    "\n",
    "    def eval(self, at):\n",
    "        # print(self.sorted_idx_1)\n",
    "        sens1 = np.array(\n",
    "            [lbl in self.sorted_idx_1[lbl][:at] for lbl in range(len(self.M))]\n",
    "        )\n",
    "        sens2 = np.array(\n",
    "            [lbl in self.sorted_idx_2[lbl][:at] for lbl in range(len(self.M))]\n",
    "        )\n",
    "\n",
    "    def get_scores(self):\n",
    "        return self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loss_func, topk, left, right_images):\n",
    "    \"\"\"Compute cosine similarity between left image and all right images in row\"\"\"\n",
    "    with torch.no_grad():\n",
    "        left_tensor = torch.tensor(np.array([left for i in range(20)]))\n",
    "        right_tensor = torch.tensor(right_images)\n",
    "        out_left, out_right = model(left_tensor.view(20,256)), model(right_tensor.view(20,256))\n",
    "\n",
    "        test_loss = loss_func(out_left, out_right, temperature, device)\n",
    "        # print(test_loss)\n",
    "        loss_dict = {\"Test/Loss\": test_loss.item()}\n",
    "\n",
    "        aR = NewAsymmetricRecall(out_left.detach().cpu(), out_right.detach().cpu())\n",
    "        scores = aR.get_scores()\n",
    "        # for k in topk:\n",
    "        #     metrics[f\"Test/top {k}\"] = aR.eval(at=k)\n",
    "        return scores\n",
    "    # return np.array([model(left, right_images[i])[0][0] for i in range(len(right_images))])\n",
    "\n",
    "\n",
    "def get_score(model, loss_func, topk,images_enc, images_names, candidates_csv):\n",
    "    \"\"\"returns a data frame with format suitable for submitting, except that values \n",
    "    don't represent probabilities but cosine similarities.\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, row in candidates_csv.iterrows():\n",
    "\n",
    "        # Compute cosine similarity between left and all other images in row\n",
    "        # print(np.where(images_names == row['left'])[0] -1)\n",
    "        # print([np.where(images_names == row[f'c{i}']) for i in range(20)])\n",
    "        left = images_enc[np.where(images_names == row['left'])[0] -1]\n",
    "        right_images = np.array([images_enc[np.where(images_names == row[f'c{i}'])] for i in range(20)])\n",
    "        sim_array = inference(model, loss_func, topk, left, right_images)\n",
    "        # print(sim_array.values())\n",
    "        res = [row['left']] + list(sim_array.tolist())\n",
    "        results.append(list(res))\n",
    "\n",
    "        if i%400 == 0 and i!=0:\n",
    "            print(f\"Processed: {i}\")\n",
    "            print(\"Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "        # break\n",
    "    results = np.array(results)\n",
    "    column_names = ['left'] + [f'c{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 256) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hec\\Documents\\UniMelb\\computer-vision\\CompVis-A4\\better-sandbox.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(images_test_pca\u001b[39m.\u001b[39mshape, \u001b[39mtype\u001b[39m(images_test_pca))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m get_score(model, inference_loss, topk, images_test_pca, test_images_names, test_candidates)\n",
      "\u001b[1;32mc:\\Users\\Hec\\Documents\\UniMelb\\computer-vision\\CompVis-A4\\better-sandbox.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m left \u001b[39m=\u001b[39m images_enc[np\u001b[39m.\u001b[39mwhere(images_names \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m right_images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([images_enc[np\u001b[39m.\u001b[39mwhere(images_names \u001b[39m==\u001b[39m row[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m])] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m)])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m sim_array \u001b[39m=\u001b[39m inference(model, loss_func, topk, left, right_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(sim_array.values())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m res \u001b[39m=\u001b[39m [row[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(sim_array\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[1;32mc:\\Users\\Hec\\Documents\\UniMelb\\computer-vision\\CompVis-A4\\better-sandbox.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m left_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray([left \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m)]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m right_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(right_images)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m out_left, out_right \u001b[39m=\u001b[39m model(left_tensor\u001b[39m.\u001b[39;49mview(\u001b[39m20\u001b[39;49m,\u001b[39m256\u001b[39;49m)), model(right_tensor\u001b[39m.\u001b[39mview(\u001b[39m20\u001b[39m,\u001b[39m256\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_loss \u001b[39m=\u001b[39m loss_func(out_left, out_right, temperature, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hec/Documents/UniMelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(test_loss)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hec\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hec\\Documents\\UniMelb\\computer-vision\\CompVis-A4\\classes\\adaptation.py:15\u001b[0m, in \u001b[0;36mAdaptation.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_matrix(x)\n\u001b[0;32m     16\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Hec\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hec\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "print(images_test_pca.shape, type(images_test_pca), )\n",
    "df = get_score(model, inference_loss, topk, images_test_pca, test_images_names, test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_i(tup, i):\n",
    "        return tup[i]\n",
    "output_df = df.copy()\n",
    "# Iterate from i = 1 to 20 and create new columns\n",
    "for i in range(20):\n",
    "    output_df[f'c{i}'] = output_df.apply(lambda row: extract_i(row[f'c{i}'], i), axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>...</th>\n",
       "      <th>c10</th>\n",
       "      <th>c11</th>\n",
       "      <th>c12</th>\n",
       "      <th>c13</th>\n",
       "      <th>c14</th>\n",
       "      <th>c15</th>\n",
       "      <th>c16</th>\n",
       "      <th>c17</th>\n",
       "      <th>c18</th>\n",
       "      <th>c19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abm</td>\n",
       "      <td>0.181642</td>\n",
       "      <td>0.113814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449350</td>\n",
       "      <td>0.463243</td>\n",
       "      <td>0.140958</td>\n",
       "      <td>0.058896</td>\n",
       "      <td>0.295562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aci</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445127</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.433957</td>\n",
       "      <td>0.107833</td>\n",
       "      <td>0.040855</td>\n",
       "      <td>0.127680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125805</td>\n",
       "      <td>0.063414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311907</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.083413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acn</td>\n",
       "      <td>0.267696</td>\n",
       "      <td>0.331837</td>\n",
       "      <td>0.483278</td>\n",
       "      <td>0.365026</td>\n",
       "      <td>0.499440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>0.164252</td>\n",
       "      <td>0.082173</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>0.058619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206016</td>\n",
       "      <td>0.170278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aco</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234525</td>\n",
       "      <td>0.096996</td>\n",
       "      <td>0.618933</td>\n",
       "      <td>0.404463</td>\n",
       "      <td>0.344928</td>\n",
       "      <td>0.477846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205659</td>\n",
       "      <td>0.071408</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616967</td>\n",
       "      <td>0.084023</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>0.196317</td>\n",
       "      <td>0.384676</td>\n",
       "      <td>0.508242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acu</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269154</td>\n",
       "      <td>0.394364</td>\n",
       "      <td>0.373098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187270</td>\n",
       "      <td>0.511895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.208351</td>\n",
       "      <td>0.542109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>zyc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.280631</td>\n",
       "      <td>0.454810</td>\n",
       "      <td>0.292498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381388</td>\n",
       "      <td>0.163035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196437</td>\n",
       "      <td>0.639109</td>\n",
       "      <td>0.102266</td>\n",
       "      <td>0.122826</td>\n",
       "      <td>0.302260</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>0.061088</td>\n",
       "      <td>0.096414</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>zyi</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103475</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.566185</td>\n",
       "      <td>0.224597</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.019888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068159</td>\n",
       "      <td>0.348148</td>\n",
       "      <td>0.145478</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>0.376220</td>\n",
       "      <td>0.887204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>zym</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599683</td>\n",
       "      <td>0.060244</td>\n",
       "      <td>0.160129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>0.309074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243469</td>\n",
       "      <td>0.056420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580711</td>\n",
       "      <td>0.156169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084407</td>\n",
       "      <td>0.257787</td>\n",
       "      <td>0.402024</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>zzq</td>\n",
       "      <td>0.009501</td>\n",
       "      <td>0.110197</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.050613</td>\n",
       "      <td>0.750223</td>\n",
       "      <td>0.400474</td>\n",
       "      <td>0.739369</td>\n",
       "      <td>0.709926</td>\n",
       "      <td>0.638798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476169</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>0.292928</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>0.735017</td>\n",
       "      <td>0.062852</td>\n",
       "      <td>0.156087</td>\n",
       "      <td>0.359236</td>\n",
       "      <td>0.068904</td>\n",
       "      <td>0.112887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>zzr</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.660159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.350321</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.054049</td>\n",
       "      <td>0.089102</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.227867</td>\n",
       "      <td>0.362267</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     left        c0        c1        c2        c3        c4        c5  \\\n",
       "0     abm  0.181642  0.113814  0.000000  0.449350  0.463243  0.140958   \n",
       "1     aci  0.000000  0.445127  0.025418  0.433957  0.107833  0.040855   \n",
       "2     acn  0.267696  0.331837  0.483278  0.365026  0.499440  0.000000   \n",
       "3     aco  0.000000  0.072751  0.000000  0.234525  0.096996  0.618933   \n",
       "4     acu  0.409283  0.000000  0.237088  0.000000  0.000000  0.000000   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  zyc  0.000000  0.182884  0.280631  0.454810  0.292498  0.000000   \n",
       "1996  zyi  0.000000  0.000000  0.103475  0.028997  0.004914  0.566185   \n",
       "1997  zym  0.000000  0.599683  0.060244  0.160129  0.000000  0.006686   \n",
       "1998  zzq  0.009501  0.110197  0.001117  0.050613  0.750223  0.400474   \n",
       "1999  zzr  0.014726  0.001344  0.660159  0.000000  0.000000  0.267093   \n",
       "\n",
       "            c6        c7        c8  ...       c10       c11       c12  \\\n",
       "0     0.058896  0.295562  0.000000  ...  0.594338  0.000000  0.164508   \n",
       "1     0.127680  0.000000  0.000000  ...  0.551866  0.000000  0.125805   \n",
       "2     0.302881  0.000000  0.406361  ...  0.022382  0.415780  0.164252   \n",
       "3     0.404463  0.344928  0.477846  ...  0.205659  0.071408  0.008396   \n",
       "4     0.269154  0.394364  0.373098  ...  0.000000  0.198789  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995  0.381388  0.163035  0.000000  ...  0.000000  0.196437  0.639109   \n",
       "1996  0.224597  0.003370  0.019888  ...  0.000279  0.000000  0.068159   \n",
       "1997  0.145412  0.065172  0.309074  ...  0.243469  0.056420  0.000000   \n",
       "1998  0.739369  0.709926  0.638798  ...  0.476169  0.102078  0.292928   \n",
       "1999  0.000000  0.000000  0.215375  ...  0.000000  0.653200  0.350321   \n",
       "\n",
       "           c13       c14       c15       c16       c17       c18       c19  \n",
       "0     0.000000  0.000000  0.000000  0.000000  0.421807  0.000000  0.370343  \n",
       "1     0.063414  0.000000  0.000000  0.311907  0.146539  0.064012  0.083413  \n",
       "2     0.082173  0.052361  0.058619  0.000000  0.000000  0.206016  0.170278  \n",
       "3     0.000000  0.616967  0.084023  0.152752  0.196317  0.384676  0.508242  \n",
       "4     0.187270  0.511895  0.000000  0.000051  0.001442  0.208351  0.542109  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1995  0.102266  0.122826  0.302260  0.242143  0.061088  0.096414  0.000000  \n",
       "1996  0.348148  0.145478  0.076007  0.047732  0.376220  0.887204  0.000000  \n",
       "1997  0.580711  0.156169  0.000000  0.084407  0.257787  0.402024  0.000000  \n",
       "1998  0.010006  0.735017  0.062852  0.156087  0.359236  0.068904  0.112887  \n",
       "1999  0.028482  0.054049  0.089102  0.155039  0.227867  0.362267  0.000000  \n",
       "\n",
       "[2000 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_output_df = make_df_as_probs(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>...</th>\n",
       "      <th>c10</th>\n",
       "      <th>c11</th>\n",
       "      <th>c12</th>\n",
       "      <th>c13</th>\n",
       "      <th>c14</th>\n",
       "      <th>c15</th>\n",
       "      <th>c16</th>\n",
       "      <th>c17</th>\n",
       "      <th>c18</th>\n",
       "      <th>c19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abm</td>\n",
       "      <td>0.049975</td>\n",
       "      <td>0.046698</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.065315</td>\n",
       "      <td>0.066229</td>\n",
       "      <td>0.047983</td>\n",
       "      <td>0.044202</td>\n",
       "      <td>0.056005</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.049126</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.063541</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.060354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aci</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.066315</td>\n",
       "      <td>0.043585</td>\n",
       "      <td>0.065579</td>\n",
       "      <td>0.047329</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>0.048278</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.048187</td>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>0.049197</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.046187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acn</td>\n",
       "      <td>0.053136</td>\n",
       "      <td>0.056656</td>\n",
       "      <td>0.065920</td>\n",
       "      <td>0.058568</td>\n",
       "      <td>0.066994</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.061040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041577</td>\n",
       "      <td>0.061617</td>\n",
       "      <td>0.047914</td>\n",
       "      <td>0.044139</td>\n",
       "      <td>0.042842</td>\n",
       "      <td>0.043111</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.049958</td>\n",
       "      <td>0.048204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aco</td>\n",
       "      <td>0.038340</td>\n",
       "      <td>0.041233</td>\n",
       "      <td>0.038340</td>\n",
       "      <td>0.048474</td>\n",
       "      <td>0.042245</td>\n",
       "      <td>0.071196</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>0.054132</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047095</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>0.038663</td>\n",
       "      <td>0.038340</td>\n",
       "      <td>0.071056</td>\n",
       "      <td>0.041701</td>\n",
       "      <td>0.044668</td>\n",
       "      <td>0.046657</td>\n",
       "      <td>0.056327</td>\n",
       "      <td>0.063735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acu</td>\n",
       "      <td>0.062461</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.054294</td>\n",
       "      <td>0.061536</td>\n",
       "      <td>0.060242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.050026</td>\n",
       "      <td>0.069211</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>0.041542</td>\n",
       "      <td>0.051091</td>\n",
       "      <td>0.071334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>zyc</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>0.054187</td>\n",
       "      <td>0.064498</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>0.059932</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>0.049812</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>0.045335</td>\n",
       "      <td>0.046277</td>\n",
       "      <td>0.055372</td>\n",
       "      <td>0.052141</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>0.045071</td>\n",
       "      <td>0.040928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>zyi</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>0.046373</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>0.042021</td>\n",
       "      <td>0.073658</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0.042655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041826</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>0.059228</td>\n",
       "      <td>0.048363</td>\n",
       "      <td>0.045117</td>\n",
       "      <td>0.043859</td>\n",
       "      <td>0.060914</td>\n",
       "      <td>0.101540</td>\n",
       "      <td>0.041815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>zym</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.076428</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.049245</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.042240</td>\n",
       "      <td>0.048525</td>\n",
       "      <td>0.044784</td>\n",
       "      <td>0.057154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053525</td>\n",
       "      <td>0.044393</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.074992</td>\n",
       "      <td>0.049050</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>0.062721</td>\n",
       "      <td>0.041958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>zzq</td>\n",
       "      <td>0.036013</td>\n",
       "      <td>0.039829</td>\n",
       "      <td>0.035713</td>\n",
       "      <td>0.037525</td>\n",
       "      <td>0.075536</td>\n",
       "      <td>0.053243</td>\n",
       "      <td>0.074721</td>\n",
       "      <td>0.072553</td>\n",
       "      <td>0.067571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057429</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.047814</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.037987</td>\n",
       "      <td>0.041699</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.039936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>zzr</td>\n",
       "      <td>0.042092</td>\n",
       "      <td>0.041532</td>\n",
       "      <td>0.080261</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.054175</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.051444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>0.058877</td>\n",
       "      <td>0.042675</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>0.048432</td>\n",
       "      <td>0.052091</td>\n",
       "      <td>0.059584</td>\n",
       "      <td>0.041477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     left        c0        c1        c2        c3        c4        c5  \\\n",
       "0     abm  0.049975  0.046698  0.041674  0.065315  0.066229  0.047983   \n",
       "1     aci  0.042491  0.066315  0.043585  0.065579  0.047329  0.044263   \n",
       "2     acn  0.053136  0.056656  0.065920  0.058568  0.066994  0.040657   \n",
       "3     aco  0.038340  0.041233  0.038340  0.048474  0.042245  0.071196   \n",
       "4     acu  0.062461  0.041482  0.052581  0.041482  0.041482  0.041482   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  zyc  0.040928  0.049141  0.054187  0.064498  0.054834  0.040928   \n",
       "1996  zyi  0.041815  0.041815  0.046373  0.043045  0.042021  0.073658   \n",
       "1997  zym  0.041958  0.076428  0.044564  0.049245  0.041958  0.042240   \n",
       "1998  zzq  0.036013  0.039829  0.035713  0.037525  0.075536  0.053243   \n",
       "1999  zzr  0.042092  0.041532  0.080261  0.041477  0.041477  0.054175   \n",
       "\n",
       "            c6        c7        c8  ...       c10       c11       c12  \\\n",
       "0     0.044202  0.056005  0.041674  ...  0.075506  0.041674  0.049126   \n",
       "1     0.048278  0.042491  0.042491  ...  0.073785  0.042491  0.048187   \n",
       "2     0.055039  0.040657  0.061040  ...  0.041577  0.061617  0.047914   \n",
       "3     0.057453  0.054132  0.061827  ...  0.047095  0.041178  0.038663   \n",
       "4     0.054294  0.061536  0.060242  ...  0.041482  0.050605  0.041482   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995  0.059932  0.048176  0.040928  ...  0.040928  0.049812  0.077550   \n",
       "1996  0.052344  0.041956  0.042655  ...  0.041826  0.041815  0.044764   \n",
       "1997  0.048525  0.044784  0.057154  ...  0.053525  0.044393  0.041958   \n",
       "1998  0.074721  0.072553  0.067571  ...  0.057429  0.039506  0.047814   \n",
       "1999  0.041477  0.041477  0.051444  ...  0.041477  0.079705  0.058877   \n",
       "\n",
       "           c13       c14       c15       c16       c17       c18       c19  \n",
       "0     0.041674  0.041674  0.041674  0.041674  0.063541  0.041674  0.060354  \n",
       "1     0.045273  0.042491  0.042491  0.058044  0.049197  0.045300  0.046187  \n",
       "2     0.044139  0.042842  0.043111  0.040657  0.040657  0.049958  0.048204  \n",
       "3     0.038340  0.071056  0.041701  0.044668  0.046657  0.056327  0.063735  \n",
       "4     0.050026  0.069211  0.041482  0.041484  0.041542  0.051091  0.071334  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1995  0.045335  0.046277  0.055372  0.052141  0.043506  0.045071  0.040928  \n",
       "1996  0.059228  0.048363  0.045117  0.043859  0.060914  0.101540  0.041815  \n",
       "1997  0.074992  0.049050  0.041958  0.045653  0.054296  0.062721  0.041958  \n",
       "1998  0.036031  0.074396  0.037987  0.041699  0.051092  0.038217  0.039936  \n",
       "1999  0.042675  0.043780  0.045342  0.048432  0.052091  0.059584  0.041477  \n",
       "\n",
       "[2000 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_output_df.to_csv('output/adaptative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
