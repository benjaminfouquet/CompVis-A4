{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2023.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ftfy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from ftfy) (0.2.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/7_/04ycmw7s02j2kfzppg4hj70h0000gn/T/pip-req-build-ax1hjrku\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/7_/04ycmw7s02j2kfzppg4hj70h0000gn/T/pip-req-build-ax1hjrku\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from clip==1.0) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from clip==1.0) (0.16.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.8)\n",
      "Requirement already satisfied: filelock in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torch->clip==1.0) (2023.9.2)\n",
      "Requirement already satisfied: numpy in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (3.5)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (4.24.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from tensorboard) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hec/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from classes.adaptation import calc_loss, construct_model\n",
    "from classes.asymmetricRecall import AsymmetricRecall\n",
    "from classes.embDataset import EmbDataset\n",
    "\n",
    "from utils.adaptationPreprocess import create_paired_embeddings_dict, merge_dicts_with_csv\n",
    "from utils.train_functions import train, validate, split_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_FILES_PATH = 'data/train/left/*'\n",
    "RIGHT_FILES_PATH = 'data/train/right/*'\n",
    "ALL_FILES_PATH = 'data/train/all/*'\n",
    "ALL_ENCODINGS_PATH = 'data/train_all_encodings.npy'\n",
    "ALL_FULL_ENCODINGS_PATH = 'data/train_all_full_encodings.npy'\n",
    "CSV_FILENAME = 'data/train.csv'\n",
    "SAVE_TO = 'output/'\n",
    "\n",
    "TEST_LEFT_FILES_PATH = 'data/test/left/*'\n",
    "TEST_RIGHT_FILES_PATH = 'data/test/right/*'\n",
    "TEST_ALL_FILES_PATH = 'data/test/all/*'\n",
    "TEST_ALL_FULL_ENCODINGS_PATH = 'data/test_all_full_encodings.npy'\n",
    "TEST_CSV_FILENAME = 'data/test_candidates.csv'\n",
    "\n",
    "train_csv = 'data/train.csv'\n",
    "train_candidates_csv = 'data/train_candidates.csv'\n",
    "test_candidates_csv = 'data/test_candidates.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if not is_cuda:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "temperature = 1\n",
    "topk = [2]\n",
    "epochs = 10\n",
    "test_split = 0.7\n",
    "model_name = 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded images\n",
    "test_images_enc = np.load('data/test_all_full_encodings.npy')\n",
    "\n",
    "# Read candidates\n",
    "test_candidates = pd.read_csv(test_candidates_csv)\n",
    "\n",
    "# Get images names\n",
    "test_images_names = np.array([x.split('.')[0] for x in os.listdir('data/test/all')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_all_full_encodings = np.load(TEST_ALL_FULL_ENCODINGS_PATH)\n",
    "\n",
    "# dimensions_before_pca = test_all_full_encodings.shape[1]\n",
    "# print('All encodings shape: ', test_all_full_encodings.shape)\n",
    "\n",
    "# test_paired_embeddings = create_paired_embeddings_dict(test_all_full_encodings, TEST_ALL_FILES_PATH, TEST_LEFT_FILES_PATH, TEST_RIGHT_FILES_PATH)\n",
    "# test_merged_dicts = merge_test_dicts_with_csv(TEST_CSV_FILENAME, test_paired_embeddings)\n",
    "# print(test_merged_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import peviously computed encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All encodings shape:  (4000, 640)\n",
      "All full encodings shape: (4000, 6080)\n"
     ]
    }
   ],
   "source": [
    "all_encodings = np.load(ALL_ENCODINGS_PATH)\n",
    "all_full_encodings = np.load(ALL_FULL_ENCODINGS_PATH)\n",
    "\n",
    "dimensions_before_pca = all_encodings.shape[1]\n",
    "print('All encodings shape: ', all_encodings.shape)\n",
    "print('All full encodings shape:', all_full_encodings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match embeddings with their original image name, link the left image with the ground truth right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 6080)\n"
     ]
    }
   ],
   "source": [
    "paired_embeddings = create_paired_embeddings_dict(all_full_encodings, ALL_FILES_PATH, LEFT_FILES_PATH, RIGHT_FILES_PATH)\n",
    "\n",
    "merged_dicts = merge_dicts_with_csv(CSV_FILENAME, paired_embeddings)\n",
    "print(merged_dicts['left'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define et construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model(model_name, dimensions_before_pca).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:   2000\n",
      "train set without aug size  300\n",
      "train set with    aug size  300\n",
      "test  set without aug size   700\n"
     ]
    }
   ],
   "source": [
    "embds_dataset = EmbDataset(merged_dicts, only_original=False)\n",
    "\n",
    "print(\"Total dataset size:  \", len(embds_dataset))\n",
    "in_dim = embds_dataset[0][0][\"left\"].shape[0]\n",
    "\n",
    "splitted = split_train_test(embds_dataset, test_split, device)\n",
    "train_set, valid_set = splitted[\"train\"], splitted[\"test\"]\n",
    "\n",
    "print(\"train set without aug size \", len(train_set[0][\"left\"]))\n",
    "print(\"train set with    aug size \", len(train_set[1][\"left\"]))\n",
    "print(\"test  set without aug size  \", len(valid_set[\"left\"]))\n",
    "\n",
    "metrics_of_all_runs = []\n",
    "\n",
    "info = (\n",
    "    \"Embd path: \"\n",
    "    + str(ALL_FULL_ENCODINGS_PATH)\n",
    "    + \"\\n\"\n",
    "    + \"Using augmented images: \"\n",
    "    + str(embds_dataset.augmented)\n",
    "    + \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, save it, and get the metrics/loss for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 / 10\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.81s/it]\n"
     ]
    }
   ],
   "source": [
    "for run in trange(runs):\n",
    "    run_path = Path(SAVE_TO) / f\"run{run+1}\"\n",
    "    model = construct_model(model_name, in_dim).to(device)\n",
    "\n",
    "    writer = SummaryWriter(run_path)\n",
    "    writer.add_text(\"Temperature: \", str(temperature))\n",
    "    writer.add_text(\"Model\", str(model).replace(\"\\n\", \"  \\n\"))\n",
    "    writer.add_text(\"Informations: \", info.replace(\"\\n\", \"  \\n\"))\n",
    "\n",
    "    if model != \"dummy\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        # writer.add_text(\"Optimizer\", str(optimizer).replace(\"\\n\", \"  \\n\"))\n",
    "\n",
    "        metrics_per_run = train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            calc_loss,\n",
    "            train_set,\n",
    "            valid_set,\n",
    "            epochs,\n",
    "            run_path,\n",
    "            temperature,\n",
    "            topk,\n",
    "            device,\n",
    "        )\n",
    "        # save the model and the optimizer state_dics for this run\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epochs,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimzier\": optimizer.state_dict(),\n",
    "            },\n",
    "            run_path / \"model_and_optimizer.pt\",\n",
    "        )\n",
    "    else:\n",
    "        metrics_per_run = validate(\n",
    "            model,\n",
    "            calc_loss,\n",
    "            valid_set,\n",
    "            temperature,\n",
    "            topk,\n",
    "            device,\n",
    "        )\n",
    "        for metric, val in metrics_per_run.items():\n",
    "            writer.add_scalar(metric, val, 1)\n",
    "            writer.flush()\n",
    "            \n",
    "\n",
    "    np.save(run_path / \"data\", metrics_per_run)\n",
    "\n",
    "    metrics_of_all_runs.append(metrics_per_run)\n",
    "\n",
    "    splitted = split_train_test(embds_dataset, test_split, device)\n",
    "    train_set, valid_set = splitted[\"train\"], splitted[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all runs metrics into one file\n",
    "np.save(Path(SAVE_TO) / \"all_runs\", metrics_of_all_runs)\n",
    "\n",
    "avg_path = Path(SAVE_TO) / \"avg\"\n",
    "writer = SummaryWriter(avg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the saved metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'Train/Loss': [5.70373010635376, 5.70353889465332, 5.703559875488281, 5.703567981719971, 5.703570365905762, 5.703568458557129, 5.703555107116699, 5.7035417556762695, 5.703525066375732, 5.7035064697265625], 'Train/top 2': [0.0033333333333333335, 0.016666666666666666, 0.056666666666666664, 0.07333333333333333, 0.09333333333333334, 0.09333333333333334, 0.1, 0.1, 0.1, 0.10666666666666667]}, 'test': {'Test/Loss': [6.551097869873047, 6.551090240478516, 6.551087856292725, 6.551086902618408, 6.55108642578125, 6.551087379455566, 6.551086902618408, 6.551088333129883, 6.551088333129883, 6.551088333129883], 'Test/top 2': [0.002857142857142857, 0.002857142857142857, 0.002857142857142857, 0.002857142857142857, 0.002857142857142857, 0.002857142857142857, 0.004285714285714286, 0.004285714285714286, 0.002857142857142857, 0.002857142857142857]}}\n"
     ]
    }
   ],
   "source": [
    "test = np.load('output/run1/data.npy', allow_pickle=True)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test_dataset, and the previously saved model. We get the prediction of the model on the test dataset, then post process it before submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with training data for validation score\n",
    "images = np.load('data/train_all_full_encodings.npy')\n",
    "pca = pickle.load(open('models/pca_module.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cos_sim_np(left, right_images):\n",
    "    \"\"\"Compute cosine similarity between left image and all right images in row\"\"\"\n",
    "    return np.array([cosine_similarity(left, right_images[i])[0][0] for i in range(len(right_images))])\n",
    "\n",
    "def cos_sim_enc(images_enc, images_names, candidates_csv):\n",
    "    \"\"\"returns a data frame with format suitable for submitting, except that values \n",
    "    don't represent probabilities but cosine similarities.\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, row in candidates_csv.iterrows():\n",
    "\n",
    "        # Compute cosine similarity between left and all other images in row\n",
    "        left = images_enc[np.where(images_names == row['left'])]\n",
    "        right_images = np.array([images_enc[np.where(images_names == row[f'c{i}'])] for i in range(20)])\n",
    "        sim_array = cos_sim_np(left, right_images)\n",
    "        res = [row['left']] + list(sim_array)\n",
    "        results.append(list(res))\n",
    "\n",
    "        if i%400 == 0 and i!=0:\n",
    "            print(f\"Processed: {i}\")\n",
    "            print(\"Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            \n",
    "    results = np.array(results)\n",
    "    column_names = ['left'] + [f'c{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Functions to make dataframes values into probabilities\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x (usually a row of dataframe).\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def make_df_as_probs(df):\n",
    "    # Make the values into probabilities with softmax\n",
    "    df_as_probs = df.apply(lambda row: softmax(np.array(row[1:]).astype(np.float64)), axis=1)\n",
    "    # Make into new dataframe\n",
    "    df_as_probs = pd.DataFrame(df_as_probs.values.tolist(), columns=df.columns[1:])\n",
    "    # Add left column in first position\n",
    "    df_as_probs.insert(0, 'left', df['left'])\n",
    "    \n",
    "    return df_as_probs\n",
    "\n",
    "# Evaluate result for training data (gives an idea of performance). For each row in df_train get \n",
    "# index of colmuns of top 2 values and check if the true match is in the top 2\n",
    "def get_score(row, row_nb, true_labels, candidates):\n",
    "    \"\"\"return 1 if true label in top_2, else return 0\"\"\"\n",
    "    # Get top 2\n",
    "    top_2 = row[1:].argsort()[-2:][::-1].values\n",
    "\n",
    "    # Get true label\n",
    "    label_row = true_labels[true_labels['left'] == row['left']]\n",
    "    true_label = label_row['right'].values[0]\n",
    "\n",
    "    # Get top 2 predicted labels. i+1 to account for 'left' column\n",
    "    top_2_names = [candidates.iloc[row_nb, i+1] for i in top_2]\n",
    "    if true_label in top_2_names:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def eval(df, true_labels, candidates):\n",
    "    \"\"\"Evaluate score on df\"\"\"\n",
    "    score = 0 \n",
    "    for i, row in df.iterrows():\n",
    "        score += get_score(row, i, true_labels, candidates)\n",
    "    return score/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded images\n",
    "train_images_enc = np.load('data/train_all_full_encodings.npy')\n",
    "test_images_enc = np.load('data/test_all_full_encodings.npy')\n",
    "\n",
    "# Read candidates\n",
    "test_candidates = pd.read_csv(test_candidates_csv)\n",
    "train_candidates = pd.read_csv(train_candidates_csv)\n",
    "\n",
    "# Get images names\n",
    "test_images_names = np.array([x.split('.')[0] for x in os.listdir('data/test/all')])\n",
    "train_images_names = np.array([x.split('.')[0] for x in os.listdir('data/train/all')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply PCA on embedded images\n",
    "images_train_pca = pca.transform(train_images_enc)\n",
    "images_test_pca = pca.transform(test_images_enc)\n",
    "\n",
    "# # Get dataframe with cosine similarities between left image and candidates\n",
    "# df_train = cos_sim_enc(images_train_pca, train_images_names, train_candidates)\n",
    "# df_test = cos_sim_enc(images_test_pca, test_images_names, test_candidates)\n",
    "\n",
    "# # Make the values into probabilities with softmax\n",
    "# df_train_as_probs = make_df_as_probs(df_train)\n",
    "# df_test_as_probs = make_df_as_probs(df_test)\n",
    "\n",
    "# # Evaluate performance on training set\n",
    "# train_labels = pd.read_csv('data/train.csv')\n",
    "# eval(df_train_as_probs, train_labels, train_candidates)\n",
    "\n",
    "# # Write csv\n",
    "# df_test_as_probs.to_csv('output/cos_sim_full_enc_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'classes.adaptation.Adaptation'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adaptation(\n",
       "  (weight_matrix): Linear(in_features=6080, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(model))\n",
    "model.eval()\n",
    "\n",
    "# output = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loss_func, topk, left, right_images):\n",
    "    \"\"\"Compute cosine similarity between left image and all right images in row\"\"\"\n",
    "    for right_image in right_images:\n",
    "        print(right_image.all() == left.all())\n",
    "        out_left, out_right = model(torch.tensor(left)), model(torch.tensor(right_image))\n",
    "        if left.all() == right_image.all():\n",
    "            print('error')\n",
    "        # print(type(left), type(right), type(out_left), type(out_right))\n",
    "        # print(left.shape, left)\n",
    "        # print(out_left, out_right)\n",
    "        test_loss = loss_func(out_left, out_right, temperature, device)\n",
    "        # print(test_loss)\n",
    "        metrics = {\"Test/Loss\": test_loss.item()}\n",
    "\n",
    "        aR = AsymmetricRecall(out_left.detach().cpu(), out_right.detach().cpu())\n",
    "\n",
    "        for k in topk:\n",
    "            metrics[f\"Test/top {k}\"] = aR.eval(at=k)\n",
    "\n",
    "        return metrics\n",
    "    return np.array([model(left, right_images[i])[0][0] for i in range(len(right_images))])\n",
    "\n",
    "\n",
    "def get_score(model, loss_func, topk,images_enc, images_names, candidates_csv):\n",
    "    \"\"\"returns a data frame with format suitable for submitting, except that values \n",
    "    don't represent probabilities but cosine similarities.\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, row in candidates_csv.iterrows():\n",
    "\n",
    "        # Compute cosine similarity between left and all other images in row\n",
    "        # print(np.where(images_names == row['left'])[0] -1)\n",
    "        # print([np.where(images_names == row[f'c{i}']) for i in range(20)])\n",
    "        left = images_enc[np.where(images_names == row['left'])[0] -1]\n",
    "        right_images = np.array([images_enc[np.where(images_names == row[f'c{i}'])] for i in range(20)])\n",
    "        print([np.where(images_names == row['left'])[0] -1])\n",
    "        print([np.where(images_names == row[f'c{i}']) for i in range(20)])\n",
    "        print(left - right_images[1])\n",
    "        sim_array = inference(model, loss_func, topk, left, right_images)\n",
    "        print(sim_array)\n",
    "        res = [row['left']] + list(sim_array)\n",
    "        results.append(list(res))\n",
    "\n",
    "        if i%400 == 0 and i!=0:\n",
    "            print(f\"Processed: {i}\")\n",
    "            print(\"Elapsed time: \", time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "        break\n",
    "    print('okkk')\n",
    "    results = np.array(results)\n",
    "    column_names = ['left'] + [f'c{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2267])]\n",
      "[(array([1378]),), (array([1462]),), (array([467]),), (array([2561]),), (array([1813]),), (array([2836]),), (array([1344]),), (array([156]),), (array([788]),), (array([2794]),), (array([2778]),), (array([3324]),), (array([2062]),), (array([1670]),), (array([3569]),), (array([2977]),), (array([3109]),), (array([413]),), (array([868]),), (array([1766]),)]\n",
      "[[ 0.          0.          0.         ...  0.01539125  0.03738412\n",
      "  -0.03599776]]\n",
      "True\n",
      "error\n",
      "{'Test/Loss': 0.0, 'Test/top 2': 1.0}\n",
      "okkk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 3), indices imply (1, 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_score(model, calc_loss, topk, test_images_enc, test_images_names, test_candidates)\n",
      "\u001b[1;32m/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(results)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m column_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m)]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(results, columns\u001b[39m=\u001b[39;49mcolumn_names)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hec/Documents/unimelb/computer-vision/CompVis-A4/better-sandbox.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages/pandas/core/frame.py:785\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    774\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    775\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    776\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    782\u001b[0m             copy\u001b[39m=\u001b[39m_copy,\n\u001b[1;32m    783\u001b[0m         )\n\u001b[1;32m    784\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 785\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    786\u001b[0m             data,\n\u001b[1;32m    787\u001b[0m             index,\n\u001b[1;32m    788\u001b[0m             columns,\n\u001b[1;32m    789\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    790\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    791\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/unimelb/computer-vision/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 3), indices imply (1, 21)"
     ]
    }
   ],
   "source": [
    "get_score(model, calc_loss, topk, test_images_enc, test_images_names, test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
